{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Self Attention without Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],\n",
    "     [0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64],\n",
    "     [0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10],\n",
    "     [0.05, 0.80, 0.55]]\n",
    ")\n",
    "# inputs with embedding dimension 3 for the six tokens\n",
    "# [\"A\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In self attention our goal is to calculate `context vectors` for each element in the input sequence. A context vector can be interpreted as an enriched embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1] # \"cat\"\n",
    "attention_scores_1 = torch.empty(inputs.shape[0])\n",
    "attention_scores_1 # attention score for word \"cat\" with respect to all other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, tokens in enumerate(inputs):\n",
    "    attention_scores_1[index] = torch.dot(query, tokens)\n",
    "    # dot product between the query and the key\n",
    "    \n",
    "attention_scores_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the attention score S21, S22, S23, S24, S25, S26 i.e we get the context of second word \"cat\" with respect to all the words in the inputs. Later we apply `Softmax` to get the `Weight Matrix` to get the `Attention Weights`.\n",
    "\n",
    "The Higher the dot product the higher the token attends to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of attention weights: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "attention_weights_1_tmp = attention_scores_1 / attention_scores_1.sum()\n",
    "\n",
    "print(f\"Attention weights: {attention_weights_1_tmp}\")\n",
    "print(f\"Sum of attention weights: {attention_weights_1_tmp.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.5971, 4.4593, 4.3728, 2.3243, 2.0279, 2.9639]), tensor(18.7453))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(attention_scores_1), torch.exp(attention_scores_1).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0) # broadcasting here [1, 6] / [1] = [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Attention weights sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "attention_weights_naive = softmax_naive(attention_scores_1)\n",
    "\n",
    "print(f\"Attention weights: {attention_weights_naive}\")\n",
    "print(f\"Attention weights sum: {attention_weights_naive.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of advantage of using softmax for calculating attention weight. They are better at handling extreme values and even handle negative cases and some numerical instability like overflow and underflow also it help in training LLM efficiently. So we use `**Softmax**`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    w = torch.softmax(x, dim=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attention_weights_1 = softmax(attention_scores_1)\n",
    "\n",
    "print(f\"Attention weights: {attention_weights_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the `Context Vectors` by multiplying embedded **input tokens** again with **attention weights** and then summing the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context vector for word cat: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # \"cat\"\n",
    "context_vector_1 = torch.zeros(query.shape) # [1, 6]\n",
    "\n",
    "for index, x_i in enumerate(inputs):\n",
    "    context_vector_1 += attention_weights_1[index] * x_i\n",
    "    \n",
    "print(f\"The context vector for word cat: {context_vector_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing everything for all words to get attention weights for all tokens in **parallel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs\n",
    "attention_scores = torch.matmul(query, query.T)\n",
    "\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9995)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(inputs[0], inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores_manually = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attention_scores_manually[i, j] = torch.dot(x_i, x_j)\n",
    "        \n",
    "attention_scores_manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention for 'A' to 'A is\t  0.999500\n",
      "Attention for 'A' to 'cat is\t  0.954400\n",
      "Attention for 'A' to 'sat is\t  0.942200\n",
      "Attention for 'A' to 'on is\t  0.475300\n",
      "Attention for 'A' to 'the is\t  0.457600\n",
      "Attention for 'A' to 'mat is\t  0.631000\n",
      "\n",
      "\n",
      "Attention for 'cat' to 'A is\t  0.954400\n",
      "Attention for 'cat' to 'cat is\t  1.495000\n",
      "Attention for 'cat' to 'sat is\t  1.475400\n",
      "Attention for 'cat' to 'on is\t  0.843400\n",
      "Attention for 'cat' to 'the is\t  0.707000\n",
      "Attention for 'cat' to 'mat is\t  1.086500\n",
      "\n",
      "\n",
      "Attention for 'sat' to 'A is\t  0.942200\n",
      "Attention for 'sat' to 'cat is\t  1.475400\n",
      "Attention for 'sat' to 'sat is\t  1.457000\n",
      "Attention for 'sat' to 'on is\t  0.829600\n",
      "Attention for 'sat' to 'the is\t  0.715400\n",
      "Attention for 'sat' to 'mat is\t  1.060500\n",
      "\n",
      "\n",
      "Attention for 'on' to 'A is\t  0.475300\n",
      "Attention for 'on' to 'cat is\t  0.843400\n",
      "Attention for 'on' to 'sat is\t  0.829600\n",
      "Attention for 'on' to 'on is\t  0.493700\n",
      "Attention for 'on' to 'the is\t  0.347400\n",
      "Attention for 'on' to 'mat is\t  0.656500\n",
      "\n",
      "\n",
      "Attention for 'the' to 'A is\t  0.457600\n",
      "Attention for 'the' to 'cat is\t  0.707000\n",
      "Attention for 'the' to 'sat is\t  0.715400\n",
      "Attention for 'the' to 'on is\t  0.347400\n",
      "Attention for 'the' to 'the is\t  0.665400\n",
      "Attention for 'the' to 'mat is\t  0.293500\n",
      "\n",
      "\n",
      "Attention for 'mat' to 'A is\t  0.631000\n",
      "Attention for 'mat' to 'cat is\t  1.086500\n",
      "Attention for 'mat' to 'sat is\t  1.060500\n",
      "Attention for 'mat' to 'on is\t  0.656500\n",
      "Attention for 'mat' to 'the is\t  0.293500\n",
      "Attention for 'mat' to 'mat is\t  0.945000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = [\"A\", \"cat\", \"sat\", \"on\",\"the\", \"mat\"]\n",
    "\n",
    "for i, attention in enumerate(attention_scores):\n",
    "    for j, word in enumerate(text):\n",
    "        print(f\"Attention for '{text[i]}' to '{text[j]} is\\t {attention[j]: .6f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalized attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs\n",
    "key = inputs\n",
    "\n",
    "attention_scores = query @ key.T\n",
    "attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weight for 'A' to 'A is\t  0.209835\n",
      "Attention weight for 'A' to 'cat is\t  0.200581\n",
      "Attention weight for 'A' to 'sat is\t  0.198149\n",
      "Attention weight for 'A' to 'on is\t  0.124228\n",
      "Attention weight for 'A' to 'the is\t  0.122049\n",
      "Attention weight for 'A' to 'mat is\t  0.145158\n",
      "\n",
      "\n",
      "Attention weight for 'cat' to 'A is\t  0.138548\n",
      "Attention weight for 'cat' to 'cat is\t  0.237891\n",
      "Attention weight for 'cat' to 'sat is\t  0.233274\n",
      "Attention weight for 'cat' to 'on is\t  0.123992\n",
      "Attention weight for 'cat' to 'the is\t  0.108182\n",
      "Attention weight for 'cat' to 'mat is\t  0.158114\n",
      "\n",
      "\n",
      "Attention weight for 'sat' to 'A is\t  0.139008\n",
      "Attention weight for 'sat' to 'cat is\t  0.236921\n",
      "Attention weight for 'sat' to 'sat is\t  0.232602\n",
      "Attention weight for 'sat' to 'on is\t  0.124204\n",
      "Attention weight for 'sat' to 'the is\t  0.110800\n",
      "Attention weight for 'sat' to 'mat is\t  0.156464\n",
      "\n",
      "\n",
      "Attention weight for 'on' to 'A is\t  0.143527\n",
      "Attention weight for 'on' to 'cat is\t  0.207394\n",
      "Attention weight for 'on' to 'sat is\t  0.204552\n",
      "Attention weight for 'on' to 'on is\t  0.146192\n",
      "Attention weight for 'on' to 'the is\t  0.126295\n",
      "Attention weight for 'on' to 'mat is\t  0.172039\n",
      "\n",
      "\n",
      "Attention weight for 'the' to 'A is\t  0.152611\n",
      "Attention weight for 'the' to 'cat is\t  0.195839\n",
      "Attention weight for 'the' to 'sat is\t  0.197491\n",
      "Attention weight for 'the' to 'on is\t  0.136687\n",
      "Attention weight for 'the' to 'the is\t  0.187859\n",
      "Attention weight for 'the' to 'mat is\t  0.129514\n",
      "\n",
      "\n",
      "Attention weight for 'mat' to 'A is\t  0.138471\n",
      "Attention weight for 'mat' to 'cat is\t  0.218364\n",
      "Attention weight for 'mat' to 'sat is\t  0.212759\n",
      "Attention weight for 'mat' to 'on is\t  0.142048\n",
      "Attention weight for 'mat' to 'the is\t  0.098806\n",
      "Attention weight for 'mat' to 'mat is\t  0.189552\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = [\"A\", \"cat\", \"sat\", \"on\",\"the\", \"mat\"]\n",
    "\n",
    "for i, attention in enumerate(attention_weights):\n",
    "    for j, word in enumerate(text):\n",
    "        print(f\"Attention weight for '{text[i]}' to '{text[j]} is\\t {attention[j]: .6f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum(dim=1) # attention weight sum to 1 in each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = inputs\n",
    "\n",
    "context_vectors = attention_weights @ value # (6 * 6) . (6 * 3) = (6 * 3)\n",
    "\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for word 'A' is  \ttensor([0.4421, 0.5931, 0.5790])\n",
      "Context vector for word 'cat' is  \ttensor([0.4419, 0.6515, 0.5683])\n",
      "Context vector for word 'sat' is  \ttensor([0.4431, 0.6496, 0.5671])\n",
      "Context vector for word 'on' is  \ttensor([0.4304, 0.6298, 0.5510])\n",
      "Context vector for word 'the' is  \ttensor([0.4671, 0.5910, 0.5266])\n",
      "Context vector for word 'mat' is  \ttensor([0.4177, 0.6503, 0.5645])\n"
     ]
    }
   ],
   "source": [
    "for i, vector in enumerate(context_vectors):\n",
    "    print(f\"Context vector for word '{text[i]}' is  \\t{vector}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
