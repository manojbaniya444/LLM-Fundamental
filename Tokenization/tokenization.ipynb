{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character level tokenization\n",
    "simple and straightforward implementation with no out of vocabulary issues and useful for task that require character level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'L', 'o', 'v', 'e', 'l', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I Lovel NLP.\"\n",
    "ch_tokens = list(text)\n",
    "print(ch_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level Tokeinzation\n",
    "Token is individual word which uses spaces and punctuation as delimeter to identify word boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Lovel', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = text.split(\" \")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Lovel', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub Word Tokenization\n",
    "Getting best out of character level and word level helps to balance vocabulary size and semantic representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Byte Pair Encoding (BPE)\n",
    "BPE start with a vocabulary of individual character and iteratively merges the most frequent adjacent pairs of characters or subwords. The process continues until a desired vocabulary size is reached.\n",
    "\n",
    "**STEPS**\n",
    "- Initialize with individual character.\n",
    "- Count the frequency of character pairs in the corpus data.\n",
    "- Merge the most frequent pair and add it to the vocabulary.\n",
    "- Repeat step 2 and 3 until desired vocabulary size is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Byte Level Byte Pair Encoding\n",
    "Used by GPT-2 model which uses bytes as the base vocabulary ensuring a fixed base vocabulary size of 256 while being able to tokenize any text without an unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=vocab_size)\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "    def train(self, files):\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'a', 'm', 'i', 'sabai', 'an', 'u', 'sa', 'a', 'san', 'ma', 'bas', 'nu', 'parcha', '.']\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = BPETokenizer()\n",
    "corpus = [\"data/text.txt\"]\n",
    "bpe_tokenizer.train(corpus)\n",
    "# after training the tokenizer on roman nepali bpe\n",
    "tokens = bpe_tokenizer.tokenize(\"Hami sabai anusaasan ma basnu parcha.\")\n",
    "# printing the\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece Tokenization\n",
    "Similar to BPE tokenization but uses a different criterion for merging tokens.\n",
    "\n",
    "It often produce more meaningful subword units, balance frequency and usefulness of token and effective for language with rich morphology but can be computationally expensize than BPE and still requires a pre-tokenization step for most implementaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=vocab_size)\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "    def train(self, files):\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.encode(text).tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', 'sabai', 'an', '##u', '##s', '##a', '##as', '##an', 'ma', 'bas', '##nu', 'parcha', '.']\n"
     ]
    }
   ],
   "source": [
    "wordpiece_tokenizer = WordPieceTokenizer()\n",
    "data = [\"data/text.txt\"]\n",
    "wordpiece_tokenizer.train(data)\n",
    "\n",
    "tokens = wordpiece_tokenizer.tokenize(\"Hami sabai anusaasan ma basnu parcha.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "Unigram start with a large vocabulary and iteratively removes tokens to reach the desired vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramTokenizerHF:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(Unigram())\n",
    "        self.trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=vocab_size)\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        \n",
    "    def train(self, files):\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tokenizer = UnigramTokenizerHF(vocab_size=1000)\n",
    "data = [\"data/text.txt\"]\n",
    "unigram_tokenizer.train(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece\n",
    "It is not a tokenization algorithm itself but rather a framework that can use BPE or Unigram algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "- BPE: general purpose\n",
    "- word-piece: for morphologically rich language\n",
    "- sentencepiece: for multilingual\n",
    "- UNIGRAM: handle ambiguity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
